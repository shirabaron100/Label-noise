# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BuJYlRjZoQ6gY246t6ybHhIhopRRT6mh

## **Visualizing CIFAR-10**
"""



"""## **cnn - label noise**"""

import keras
import tensorflow as tf
from keras.datasets import cifar10
from tensorflow import keras
import numpy as np
import random
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import RMSprop

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

num_classes = 10 #define the number of classes possible.

########################################
# Symmetric noise: randomly choose x% from the labels and randomly switch their labels

# Train with x% label noise

# Repeat for the other noise levels

# Compare model.evaluate(x_test, y_test) for each noise level and baseline (without noise)

y_10_train = y_train
y_20_train = y_train
y_30_train = y_train
y_40_train = y_train
y_50_train = y_train

for j in range(5000):
  y_10_train[int(j)]= random.randint(0, 9)

for j in range(10000):
  y_20_train[int(j)]= random.randint(0, 9)

for j in range(15000):
  y_30_train[int(j)]= random.randint(0, 9)

for j in range(20000):
  y_40_train[int(j)]= random.randint(0, 9)

for j in range(25000):
  y_50_train[int(j)]= random.randint(0, 9)


# Convert class vectors to binary class matrices.
y_train_1 = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
y_10_train = keras.utils.to_categorical(y_10_train, num_classes)
y_20_train = keras.utils.to_categorical(y_20_train, num_classes)
y_30_train = keras.utils.to_categorical(y_30_train, num_classes)
y_40_train = keras.utils.to_categorical(y_40_train, num_classes)
y_50_train = keras.utils.to_categorical(y_50_train, num_classes)

#Normalize
x_train  = x_train / 255.0
x_test = x_test / 255.0

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9152):
      print("\nReached 91.0% accuracy cancel any further epochs!")
      print(logs)
      self.model.stop_training = True

callbacks = myCallback()

####### baseline_model ###########
print()
print("############### baseline_model ##############")
baseline_model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])



baseline_model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
#model.fit(x_train, y_train, epochs=20, callbacks=[callbacks])
history=baseline_model.fit(x_train, y_train_1, epochs=20, callbacks=[callbacks])


####### model 10 ###########

print()
print("############### model 10 %##############")
model_10 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_10.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_10 = model_10.fit(x_train, y_10_train, epochs=20, callbacks=[callbacks])


model_10.evaluate(x_test, y_test)

####### model 20 ###########

print()
print("############### model 20 %##############")
model_20 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_20.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_20 = model_20.fit(x_train, y_20_train, epochs=20, callbacks=[callbacks])


model_20.evaluate(x_test, y_test)

####### model 30 ###########

print()
print("############### model 30 %##############")
model_30 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_30.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_30 = model_30.fit(x_train, y_30_train, epochs=20, callbacks=[callbacks])


model_30.evaluate(x_test, y_test)


####### model 40 ###########

print()
print("############### model 40 %##############")
model_40 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_40.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_40 = model_40.fit(x_train, y_40_train, epochs=20, callbacks=[callbacks])


model_40.evaluate(x_test, y_test)


####### model 50 ###########

print()
print("############### model 50 %##############")
model_50 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_50.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_50 = model_50.fit(x_train, y_50_train, epochs=20, callbacks=[callbacks])

model_50.evaluate(x_test, y_test)

#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history_10.history['accuracy'])
plt.plot(history_20.history['accuracy'])
plt.plot(history_30.history['accuracy'])
plt.plot(history_40.history['accuracy'])
plt.plot(history_50.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['baseline', '10% label noise','20% label noise','30% label noise','40% label noise','50% label noise'], loc='upper left')
plt.show()

#########################################################################
#0<-->5
#1<-->6
#2<-->8
#3<-->7
#4<-->9

def changeTheLabel(labelNoise, num):
  for i in range(num):
    if(labelNoise[i]==0):
      labelNoise[i]=5
    elif (labelNoise[i]==1):
      labelNoise[i]=6
    elif (labelNoise[i]==2):
      labelNoise[i]=8
    elif (labelNoise[i]==3):
      labelNoise[i]=7
    elif (labelNoise[i]==4):
      labelNoise[i]=9
    elif (labelNoise[i]==5):
      labelNoise[i]=0
    elif (labelNoise[i]==6):
      labelNoise[i]=1
    elif (labelNoise[i]==7):
      labelNoise[i]=3
    elif (labelNoise[i]==8):
      labelNoise[i]=2
    elif (labelNoise[i]==9):
      labelNoise[i]=4
  return labelNoise

########################################
# Asymmetric noise: randomply choose x% from the labels and swap classes according to some heuristics you define(1<-->5, 2<-->6, etc)

# Train with x% label noise

# Repeat for the other noise levels

# Compare model.evaluate(x_test, y_test) for each noise level and baseline (without noise)



y_10_train = y_train
y_10_train = changeTheLabel(y_10_train,5000)
y_20_train = y_train
y_20_train = changeTheLabel(y_20_train,10000)
y_30_train = y_train
y_30_train = changeTheLabel(y_30_train,15000)
y_40_train = y_train
y_40_train = changeTheLabel(y_40_train,20000)
y_50_train = y_train
y_50_train = changeTheLabel(y_50_train,25000)



# Convert class vectors to binary class matrices.
y_10_train = keras.utils.to_categorical(y_10_train, num_classes)
y_20_train = keras.utils.to_categorical(y_20_train, num_classes)
y_30_train = keras.utils.to_categorical(y_30_train, num_classes)
y_40_train = keras.utils.to_categorical(y_40_train, num_classes)
y_50_train = keras.utils.to_categorical(y_50_train, num_classes)



####### model 10 ###########

print()
print("############### model 10 %##############")
model_10 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_10.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_10 = model_10.fit(x_train, y_10_train, epochs=20, callbacks=[callbacks])


model_10.evaluate(x_test, y_test)

####### model 20 ###########

print()
print("############### model 20 %##############")
model_20 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_20.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_20 = model_20.fit(x_train, y_20_train, epochs=20, callbacks=[callbacks])


model_20.evaluate(x_test, y_test)

####### model 10 ###########

print()
print("############### model 30 %##############")
model_30 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_30.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_30 = model_30.fit(x_train, y_30_train, epochs=20, callbacks=[callbacks])


model_30.evaluate(x_test, y_test)


####### model 40 ###########

print()
print("############### model 40 %##############")
model_40 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_40.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_40 = model_40.fit(x_train, y_40_train, epochs=20, callbacks=[callbacks])


model_40.evaluate(x_test, y_test)


####### model 50 ###########

print()
print("############### model 50 %##############")
model_50 = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                   tf.keras.layers.Dense(1024, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

model_50.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


history_50 = model_50.fit(x_train, y_50_train, epochs=20, callbacks=[callbacks])

model_50.evaluate(x_test, y_test)

#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history_10.history['accuracy'])
plt.plot(history_20.history['accuracy'])
plt.plot(history_30.history['accuracy'])
plt.plot(history_40.history['accuracy'])
plt.plot(history_50.history['accuracy'])
plt.title('CNN Asymetric')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['baseline', '10% label noise','20% label noise','30% label noise','40% label noise','50% label noise'], loc='upper left')
plt.show()

"""## **random forest - label noise**"""

## Image classification using RandomForest: An example in Python using CIFAR10 Dataset


print(format('Image classification using RandomForest: An example in Python using CIFAR10 Dataset','*^88'))

import warnings
warnings.filterwarnings("ignore")    
    
# load libraries
from keras.datasets import cifar10
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score    
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import random
import time
import copy 
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import collections as matcoll


start_time = time.time()

# data: shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# X_train is 50000 rows of 3x32x32 values --> reshaped in 50000 x 3072
RESHAPED = 3072

X_train = X_train.reshape(50000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

y_train = y_train.flatten()
y_test = y_test.flatten()

y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)

for i in range(5000):
 y_10_train[i] = random.randint(0, 9)
for i in range(10000):
 y_20_train[i] = random.randint(0, 9)
for i in range(15000):
 y_30_train[i] = random.randint(0, 9)
for i in range(20000):
 y_40_train[i] = random.randint(0, 9)
for i in range(25000):
 y_50_train[i] = random.randint(0, 9)
# normalize the datasets
X_train /= 255.
X_test /= 255.
    
    
#print(X_train.shape[0], 'train samples')
#print(X_test.shape[0], 'test samples')
    
# fit a RandomForest model to the data
baseline_model = RandomForestClassifier(n_estimators = 10)
model_10 = RandomForestClassifier(n_estimators = 10)
model_20 = RandomForestClassifier(n_estimators = 10)
model_30 = RandomForestClassifier(n_estimators = 10)
model_40 = RandomForestClassifier(n_estimators = 10)
model_50 = RandomForestClassifier(n_estimators = 10)

baseline_model.fit(X_train, y_train)
model_10.fit(X_train,y_10_train)
model_20.fit(X_train,y_20_train)
model_30.fit(X_train,y_30_train)
model_40.fit(X_train,y_40_train)
model_50.fit(X_train,y_50_train)

#print(); print(cv_results)    
#print(); print(model)

# make predictions
expected_y  = y_test
predicted_y = baseline_model.predict(X_test)
predicted_10_y = model_10.predict(X_test)
predicted_20_y = model_20.predict(X_test)
predicted_30_y = model_30.predict(X_test)
predicted_40_y = model_40.predict(X_test)
predicted_50_y = model_50.predict(X_test)

# summarize the fit of the model
# print(); print(metrics.classification_report(expected_y, predicted_y))
# print(); print(metrics.confusion_matrix(expected_y, predicted_y))
print(accuracy_score(predicted_y, expected_y))
print(accuracy_score(predicted_10_y, expected_y))
print(accuracy_score(predicted_20_y, expected_y))
print(accuracy_score(predicted_30_y, expected_y))
print(accuracy_score(predicted_40_y, expected_y))
print(accuracy_score(predicted_50_y, expected_y))

x = np.arange(1,7)
y = [accuracy_score(predicted_y, expected_y),accuracy_score(predicted_10_y, expected_y),accuracy_score(predicted_20_y, expected_y)
,accuracy_score(predicted_30_y, expected_y),accuracy_score(predicted_40_y, expected_y),accuracy_score(predicted_50_y, expected_y)]



# print("Execution Time %s seconds: " % (time.time() - start_time))    


y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)

y_10_train = y_train
y_10_train = changeTheLabel(y_10_train,5000)
y_20_train = y_train
y_20_train = changeTheLabel(y_20_train,10000)
y_30_train = y_train
y_30_train = changeTheLabel(y_30_train,15000)
y_40_train = y_train
y_40_train = changeTheLabel(y_40_train,20000)
y_50_train = y_train
y_50_train = changeTheLabel(y_50_train,25000)



# normalize the datasets
X_train /= 255.
X_test /= 255.
    
    
#print(X_train.shape[0], 'train samples')
#print(X_test.shape[0], 'test samples')
    
# fit a RandomForest model to the data
model_10 = RandomForestClassifier(n_estimators = 10)
model_20 = RandomForestClassifier(n_estimators = 10)
model_30 = RandomForestClassifier(n_estimators = 10)
model_40 = RandomForestClassifier(n_estimators = 10)
model_50 = RandomForestClassifier(n_estimators = 10)

model_10.fit(X_train,y_10_train)
model_20.fit(X_train,y_20_train)
model_30.fit(X_train,y_30_train)
model_40.fit(X_train,y_40_train)
model_50.fit(X_train,y_50_train)

#print(); print(cv_results)    
#print(); print(model)

# make predictions
expected_y  = y_test
predicted_10_y = model_10.predict(X_test)
predicted_20_y = model_20.predict(X_test)
predicted_30_y = model_30.predict(X_test)
predicted_40_y = model_40.predict(X_test)
predicted_50_y = model_50.predict(X_test)

# summarize the fit of the model
# print(); print(metrics.classification_report(expected_y, predicted_y))
# print(); print(metrics.confusion_matrix(expected_y, predicted_y))
print(accuracy_score(predicted_y, expected_y))
print(accuracy_score(predicted_10_y, expected_y))
print(accuracy_score(predicted_20_y, expected_y))
print(accuracy_score(predicted_30_y, expected_y))
print(accuracy_score(predicted_40_y, expected_y))
print(accuracy_score(predicted_50_y, expected_y))

x_asi = np.arange(1,7)
y_asi = [accuracy_score(predicted_y, expected_y),accuracy_score(predicted_10_y, expected_y),accuracy_score(predicted_20_y, expected_y)
,accuracy_score(predicted_30_y, expected_y),accuracy_score(predicted_40_y, expected_y),accuracy_score(predicted_50_y, expected_y)]

my_xticks = ['baise line','10% noise','20% noise','30% noise','40% noise','50% noise']
plt.xticks(x_asi, my_xticks)
plt.plot(x, y,'r')
plt.plot(x_asi, y_asi,'b')
plt.title('Random Forest')
plt.ylabel('accuracy')
plt.xlabel('noise')
plt.legend(['simetric','asimetric'], loc='upper left')
plt.show()

# print("Execution Time %s seconds: " % (time.time() - start_time))

"""## **Logistic Regression- label noise**"""

from sklearn.linear_model import LogisticRegression
from keras.datasets import cifar10
import random
import copy 
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt


# load dataset
(x_train, y_train), (x_test , y_test) = cifar10.load_data()

x_train_flat = x_train.reshape(x_train.shape[0], x_train.shape[1]* x_train.shape[2]* x_train.shape[3])
x_test_flat = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2] * x_test.shape[3])

y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)

for i in range(5000):
 y_10_train[i] = random.randint(0, 9)
for i in range(10000):
 y_20_train[i] = random.randint(0, 9)
for i in range(15000):
 y_30_train[i] = random.randint(0, 9)
for i in range(20000):
 y_40_train[i] = random.randint(0, 9)
for i in range(25000):
 y_50_train[i] = random.randint(0, 9)
# # normalize the datasets
# x_train_flat /= 255.
# x_test_flat /= 255.

# fit a RandomForest model to the data
baseline_model = LogisticRegression()
model_10 = LogisticRegression()
model_20 = LogisticRegression()
model_30 = LogisticRegression()
model_40 = LogisticRegression()
model_50 = LogisticRegression()


baseline_model.fit(x_train_flat , y_train)
model_10.fit(x_train_flat , y_10_train)
model_20.fit(x_train_flat , y_20_train)
model_30.fit(x_train_flat , y_30_train)
model_40.fit(x_train_flat , y_40_train)
model_50.fit(x_train_flat , y_50_train)

# Returns a NumPy Array
# Predict for One Observation (image)
baseline_model.predict(x_test_flat[0].reshape(1,-1))
model_10.predict(x_test_flat[0].reshape(1,-1))
model_20.predict(x_test_flat[0].reshape(1,-1))
model_30.predict(x_test_flat[0].reshape(1,-1))
model_40.predict(x_test_flat[0].reshape(1,-1))
model_50.predict(x_test_flat[0].reshape(1,-1))

baseline_model.predict(x_test_flat[0:10])
model_10.predict(x_test_flat[0:10])
model_20.predict(x_test_flat[0:10])
model_30.predict(x_test_flat[0:10])
model_40.predict(x_test_flat[0:10])
model_50.predict(x_test_flat[0:10])

predictions = baseline_model.predict(x_test_flat)
predictions_10 = model_10.predict(x_test_flat)
predictions_20 = model_20.predict(x_test_flat)
predictions_30 = model_30.predict(x_test_flat)
predictions_40 = model_40.predict(x_test_flat)
predictions_50 = model_50.predict(x_test_flat)

# Use score method to get accuracy of model
score = baseline_model.score(x_test_flat, y_test)
score_10 = model_10.score(x_test_flat, y_test)
score_20 = model_20.score(x_test_flat, y_test)
score_30 = model_30.score(x_test_flat, y_test)
score_40 = model_40.score(x_test_flat, y_test)
score_50 = model_50.score(x_test_flat, y_test)

print(score)
print(score_10)
print(score_20)
print(score_30)
print(score_40)
print(score_50)

x = np.arange(1,7)
y = [accuracy_score(predictions, y_test),accuracy_score(predictions_10 , y_test),accuracy_score(predictions_20 , y_test),accuracy_score(predictions_30 , y_test),accuracy_score(predictions_40 , y_test),accuracy_score(predictions_50 , y_test)]

y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)

y_10_train = changeTheLabel(y_10_train,5000)
y_20_train = changeTheLabel(y_20_train,10000)
y_30_train = changeTheLabel(y_30_train,15000)
y_40_train = changeTheLabel(y_40_train,20000)
y_50_train = changeTheLabel(y_50_train,25000)

#print(X_train.shape[0], 'train samples')
#print(X_test.shape[0], 'test samples')
    

model_10 = LogisticRegression()
model_20 = LogisticRegression()
model_30 = LogisticRegression()
model_40 = LogisticRegression()
model_50 = LogisticRegression()

model_10.fit(x_train_flat , y_10_train)
model_20.fit(x_train_flat , y_20_train)
model_30.fit(x_train_flat , y_30_train)
model_40.fit(x_train_flat , y_40_train)
model_50.fit(x_train_flat , y_50_train)


#print(); print(cv_results)    
#print(); print(model)



predictions = baseline_model.predict(x_test_flat)
predictions_10 = model_10.predict(x_test_flat)
predictions_20 = model_20.predict(x_test_flat)
predictions_30 = model_30.predict(x_test_flat)
predictions_40 = model_40.predict(x_test_flat)
predictions_50 = model_50.predict(x_test_flat)

# Use score method to get accuracy of model
score = baseline_model.score(x_test_flat, y_test)
score_10 = model_10.score(x_test_flat, y_test)
score_20 = model_20.score(x_test_flat, y_test)
score_30 = model_30.score(x_test_flat, y_test)
score_40 = model_40.score(x_test_flat, y_test)
score_50 = model_50.score(x_test_flat, y_test)

print(score)
print(score_10)
print(score_20)
print(score_30)
print(score_40)
print(score_50)

x_asi = np.arange(1,7)
y_asi = [accuracy_score(predictions, y_test),accuracy_score(predictions_10 , y_test),accuracy_score(predictions_20 , y_test),accuracy_score(predictions_30 , y_test),accuracy_score(predictions_40 , y_test),accuracy_score(predictions_50 , y_test)]

my_xticks = ['baise line','10% noise','20% noise','30% noise','40% noise','50% noise']
plt.xticks(x_asi, my_xticks)
plt.plot(x, y,'r')
plt.plot(x_asi, y_asi,'b')
plt.title('Logistic Regression')
plt.ylabel('accuracy')
plt.xlabel('noise')
plt.legend(['symetric','asymetric'], loc='upper left')
plt.show()

x_asi = np.arange(1,7)
y_asi = [accuracy_score(predictions, y_test),accuracy_score(predictions_10 , y_test),accuracy_score(predictions_20 , y_test),accuracy_score(predictions_30 , y_test),accuracy_score(predictions_40 , y_test),accuracy_score(predictions_50 , y_test)]

my_xticks = ['baise line','10% noise','20% noise','30% noise','40% noise','50% noise']
plt.xticks(x_asi, my_xticks)
plt.plot(x, y,'r')
plt.plot(x_asi, y_asi,'b')
plt.title('Logistic Regression')
plt.ylabel('accuracy')
plt.xlabel('noise')
plt.legend(['symetric','asymetric'], loc='upper left')
plt.show()

"""## **AdaBoost - label noise**"""

# Load libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn import datasets
# Import train_test_split function
from sklearn.model_selection import train_test_split
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
from keras.datasets import cifar10
import copy 
import random
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load data
(X_train, y_train), (X_test , y_test) = cifar10.load_data()
 
X_train_flat = X_train.reshape(X_train.shape[0], X_train.shape[1]* X_train.shape[2]* X_train.shape[3])
X_test_flat = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2] * X_test.shape[3])
 
y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)
 
for i in range(1000):
 y_10_train[i] = random.randint(0, 9)
for i in range(10000):
 y_20_train[i] = random.randint(0, 9)
for i in range(15000):
 y_30_train[i] = random.randint(0, 9)
for i in range(25000):
 y_40_train[i] = random.randint(0, 9)
for i in range(40000):
 y_50_train[i] = random.randint(0, 9)
 
# Create adaboost classifer object
a = AdaBoostClassifier(n_estimators=50,learning_rate=1)
B = AdaBoostClassifier(n_estimators=50,learning_rate=1)
C = AdaBoostClassifier(n_estimators=50,learning_rate=1)
D = AdaBoostClassifier(n_estimators=50,learning_rate=1)
E = AdaBoostClassifier(n_estimators=50,learning_rate=1)
F = AdaBoostClassifier(n_estimators=50,learning_rate=1)
 
# Train Adaboost Classifer
baseline_model = a.fit(X_train_flat, y_train)
model_10 = B.fit(X_train_flat, y_10_train)
model_20 = C.fit(X_train_flat, y_20_train)
model_30 = D.fit(X_train_flat, y_30_train)
model_40 = E.fit(X_train_flat, y_40_train)
model_50 = F.fit(X_train_flat, y_50_train)
 
#Predict the response for test dataset
y_pred = baseline_model.predict(X_test_flat)
y_pred_10 = model_10.predict(X_test_flat)
y_pred_20 = model_20.predict(X_test_flat)
y_pred_30 = model_30.predict(X_test_flat)
y_pred_40 = model_40.predict(X_test_flat)
y_pred_50 = model_50.predict(X_test_flat)
 
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_10))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_20))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_30))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_40))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_50))

x = np.arange(1,7)
y = [accuracy_score(y_pred, y_test),accuracy_score(y_pred_10, y_test),accuracy_score(y_pred_20, y_test)
,accuracy_score(y_pred_30, y_test),accuracy_score(y_pred_40, y_test),accuracy_score(y_pred_50, y_test)]

def changeTheLabel(labelNoise, num):
  for i in range(num):
    if(labelNoise[i]==0):
      labelNoise[i]=5
    elif (labelNoise[i]==1):
      labelNoise[i]=6
    elif (labelNoise[i]==2):
      labelNoise[i]=8
    elif (labelNoise[i]==3):
      labelNoise[i]=7
    elif (labelNoise[i]==4):
      labelNoise[i]=9
    elif (labelNoise[i]==5):
      labelNoise[i]=0
    elif (labelNoise[i]==6):
      labelNoise[i]=1
    elif (labelNoise[i]==7):
      labelNoise[i]=3
    elif (labelNoise[i]==8):
      labelNoise[i]=2
    elif (labelNoise[i]==9):
      labelNoise[i]=4
  return labelNoise


y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)
 
y_10_train = changeTheLabel(y_10_train,5000)
y_20_train = changeTheLabel(y_20_train,10000)
y_30_train = changeTheLabel(y_30_train,15000)
y_40_train = changeTheLabel(y_40_train,20000)
y_50_train = changeTheLabel(y_50_train,25000)
 
# Create adaboost classifer object
a = AdaBoostClassifier(n_estimators=50,learning_rate=1)
B = AdaBoostClassifier(n_estimators=50,learning_rate=1)
C = AdaBoostClassifier(n_estimators=50,learning_rate=1)
D = AdaBoostClassifier(n_estimators=50,learning_rate=1)
E = AdaBoostClassifier(n_estimators=50,learning_rate=1)
F = AdaBoostClassifier(n_estimators=50,learning_rate=1)
 
# Train Adaboost Classifer
baseline_model = a.fit(X_train_flat, y_train)
model_10 = B.fit(X_train_flat, y_10_train)
model_20 = C.fit(X_train_flat, y_20_train)
model_30 = D.fit(X_train_flat, y_30_train)
model_40 = E.fit(X_train_flat, y_40_train)
model_50 = F.fit(X_train_flat, y_50_train)
 
#Predict the response for test dataset
y_pred = baseline_model.predict(X_test_flat)
y_pred_10 = model_10.predict(X_test_flat)
y_pred_20 = model_20.predict(X_test_flat)
y_pred_30 = model_30.predict(X_test_flat)
y_pred_40 = model_40.predict(X_test_flat)
y_pred_50 = model_50.predict(X_test_flat)
 
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_10))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_20))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_30))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_40))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_50))

x_asi = np.arange(1,7)
y_asi = [accuracy_score(y_pred, y_test),accuracy_score(y_pred_10, y_test),accuracy_score(y_pred_20, y_test)
,accuracy_score(y_pred_30, y_test),accuracy_score(y_pred_40, y_test),accuracy_score(y_pred_50, y_test)]

import matplotlib.pyplot as plt
my_xticks = ['baise line','10% noise','20% noise','30% noise','40% noise','50% noise']
plt.xticks(x_asi, my_xticks)
plt.plot(x, y,'r')
plt.plot(x_asi, y_asi,'b')
plt.title('AdaBoost')
plt.ylabel('accuracy')
plt.xlabel('noise')
plt.legend(['symetric','asymetric'], loc='upper left')
plt.show()

# print("Execution Time %s seconds: " % (time.time() - start_time))

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
x = np.arange(1,7)
y = [accuracy_score(y_pred, y_test),accuracy_score(y_pred_10, y_test),accuracy_score(y_pred_20, y_test)
,accuracy_score(y_pred_30, y_test),accuracy_score(y_pred_40, y_test),accuracy_score(y_pred_50, y_test)]

def changeTheLabel(labelNoise, num):
  for i in range(num):
    if(labelNoise[i]==0):
      labelNoise[i]=5
    elif (labelNoise[i]==1):
      labelNoise[i]=6
    elif (labelNoise[i]==2):
      labelNoise[i]=8
    elif (labelNoise[i]==3):
      labelNoise[i]=7
    elif (labelNoise[i]==4):
      labelNoise[i]=9
    elif (labelNoise[i]==5):
      labelNoise[i]=0
    elif (labelNoise[i]==6):
      labelNoise[i]=1
    elif (labelNoise[i]==7):
      labelNoise[i]=3
    elif (labelNoise[i]==8):
      labelNoise[i]=2
    elif (labelNoise[i]==9):
      labelNoise[i]=4
  return labelNoise


y_10_train = copy.deepcopy(y_train)
y_20_train = copy.deepcopy(y_train)
y_30_train = copy.deepcopy(y_train)
y_40_train = copy.deepcopy(y_train)
y_50_train = copy.deepcopy(y_train)
 
y_10_train = changeTheLabel(y_10_train,5000)
y_20_train = changeTheLabel(y_20_train,10000)
y_30_train = changeTheLabel(y_30_train,15000)
y_40_train = changeTheLabel(y_40_train,20000)
y_50_train = changeTheLabel(y_50_train,25000)
 
# Create adaboost classifer object
a = AdaBoostClassifier(n_estimators=50,learning_rate=1)
B = AdaBoostClassifier(n_estimators=50,learning_rate=1)
C = AdaBoostClassifier(n_estimators=50,learning_rate=1)
D = AdaBoostClassifier(n_estimators=50,learning_rate=1)
E = AdaBoostClassifier(n_estimators=50,learning_rate=1)
F = AdaBoostClassifier(n_estimators=50,learning_rate=1)
 
# Train Adaboost Classifer
baseline_model = a.fit(X_train_flat, y_train)
model_10 = B.fit(X_train_flat, y_10_train)
model_20 = C.fit(X_train_flat, y_20_train)
model_30 = D.fit(X_train_flat, y_30_train)
model_40 = E.fit(X_train_flat, y_40_train)
model_50 = F.fit(X_train_flat, y_50_train)
 
#Predict the response for test dataset
y_pred = baseline_model.predict(X_test_flat)
y_pred_10 = model_10.predict(X_test_flat)
y_pred_20 = model_20.predict(X_test_flat)
y_pred_30 = model_30.predict(X_test_flat)
y_pred_40 = model_40.predict(X_test_flat)
y_pred_50 = model_50.predict(X_test_flat)
 
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_10))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_20))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_30))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_40))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred_50))

x_asi = np.arange(1,7)
y_asi = [accuracy_score(y_pred, y_test),accuracy_score(y_pred_10, y_test),accuracy_score(y_pred_20, y_test)
,accuracy_score(y_pred_30, y_test),accuracy_score(y_pred_40, y_test),accuracy_score(y_pred_50, y_test)]

import matplotlib.pyplot as plt
my_xticks = ['baise line','10% noise','20% noise','30% noise','40% noise','50% noise']
plt.xticks(x_asi, my_xticks)
plt.plot(x, y,'r')
plt.plot(x_asi, y_asi,'b')
plt.title('AdaBoost')
plt.ylabel('accuracy')
plt.xlabel('noise')
plt.legend(['symetric','asymetric'], loc='upper left')
plt.show()

# print("Execution Time %s seconds: " % (time.time() - start_time))